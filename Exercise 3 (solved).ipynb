{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 30 â€” Sep 3, 2021<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CSV file\n",
    "data = pd.read_csv(\"data/ner/gmb.csv\", encoding = 'latin1')\n",
    "\n",
    "# the first column of the file contains the sentence number\n",
    "# -- but only for the first token of each sentence.\n",
    "# The following line fills the rows downwards.\n",
    "data = data.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words and assign an integer number to it\n",
    "unique_words, coded_words = np.unique(data[\"Word\"], return_inverse=True)\n",
    "data[\"Word_idx\"] = coded_words\n",
    "EMPTY_WORD_IDX = len(unique_words)\n",
    "np.array(unique_words.tolist().append(\"_____\"))\n",
    "num_words = len(unique_words)+1\n",
    "\n",
    "unique_pos_tags, coded_pos_tags = np.unique(data[\"POS\"], return_inverse=True)\n",
    "data[\"POS_idx\"]  = coded_pos_tags\n",
    "NO_POS_TAG_IDX = len(unique_pos_tags)\n",
    "unique_pos_tags = unique_pos_tags.tolist()\n",
    "unique_pos_tags.append(\"_\")\n",
    "unique_pos_tags = np.array(unique_pos_tags)\n",
    "num_pos_tags = len(unique_pos_tags)\n",
    "\n",
    "\n",
    "# create a list of unique tags and assign an integer number to it\n",
    "unique_ne_tags, coded_ne_tags = np.unique(data[\"Tag\"], return_inverse=True)\n",
    "data[\"NE_idx\"]  = coded_ne_tags\n",
    "NO_NE_TAG_IDX = unique_ne_tags.tolist().index(\"O\")\n",
    "num_ne_tags = len(unique_ne_tags)\n",
    "\n",
    "# for verification and inspection, we can inspect the table so far\n",
    "data[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in sentence-wise processing.\n",
    "# Therefore, we use a function that gives us individual sentences.\n",
    "def get_sentences(data):\n",
    "  n_sent=1\n",
    "  agg_func = lambda s:[(w,p,t) \n",
    "    for w,p,t in zip(\n",
    "      s[\"Word_idx\"].values.tolist(),\n",
    "      s[\"POS_idx\"].values.tolist(),\n",
    "      s[\"NE_idx\"].values.tolist())]\n",
    "  grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "  return [s for s in grouped]\n",
    "\n",
    "sentences = get_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# find the maximum length for the sentences\n",
    "max_len = max([len(s) for s in sentences])\n",
    "\n",
    "# extract the word index\n",
    "x = [ [ w[0] for w in s ] for s in sentences ]\n",
    "\n",
    "# extract the tag index\n",
    "y_pos = [ [ w[1] for w in s ] for s in sentences ]\n",
    "y_ne = [ [ w[2] for w in s ] for s in sentences ]\n",
    "\n",
    "# shorter sentences are now padded to same length, using (index of) padding symbol\n",
    "x = pad_sequences(maxlen = max_len, sequences = x, \n",
    "  padding = 'post', value = EMPTY_WORD_IDX)\n",
    "\n",
    "# we do the same for the y data\n",
    "y_ne = pad_sequences(maxlen = max_len, sequences = y_ne, \n",
    "  padding = 'post', value = NO_NE_TAG_IDX)\n",
    "y_pos = pad_sequences(maxlen = max_len, sequences = y_pos, \n",
    "  padding = 'post', value = NO_POS_TAG_IDX)\n",
    "\n",
    "y_ne = np.array(y_ne)\n",
    "y_pos = np.array(y_pos)\n",
    "\n",
    "# but we also convert the indices to one-hot-encoding\n",
    "y_ne = to_categorical(y_ne, num_classes = num_ne_tags)\n",
    "y_pos = to_categorical(y_pos, num_classes = num_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_ne_train,y_ne_test,train_indices,test_indices = train_test_split(x, y_ne, range(len(x)), test_size = 0.1, random_state=1)\n",
    "\n",
    "y_pos_train = y_pos[train_indices]\n",
    "y_pos_test = y_pos[test_indices]\n",
    "\n",
    "y_train_weights = np.array([ [ 0.1 if w[len(w)-1] == 1 else 1 for w in s ]  for s in y_ne_train ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "l_input = layers.Input(shape = (max_len,))\n",
    "l_embedding = layers.Embedding(input_dim = num_words, output_dim = 50, input_length = max_len)(l_input)\n",
    "l_lstm = layers.LSTM(units = 5, return_sequences = True)(l_embedding)\n",
    "l_output_ne = layers.Dense(num_ne_tags, name=\"ne\", activation = 'softmax')(l_lstm)\n",
    "l_output_pos = layers.Dense(num_pos_tags, name=\"pos\", activation = 'softmax')(l_lstm)\n",
    "\n",
    "model = models.Model(inputs = l_input, outputs=[l_output_ne, l_output_pos])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# We use a different optimizer this time\n",
    "model.compile(optimizer='Adam', \n",
    "  loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, [np.array(y_ne_train), np.array(y_pos_train)],\n",
    "    batch_size = 64,\n",
    "    epochs = 2,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, [y_ne_test, y_pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse one-hot-encoding for test data\n",
    "y_ne_test = np.argmax(y_ne_test, axis=2)\n",
    "y_pos_test = np.argmax(y_pos_test, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_ne_pred, y_pos_pred = model.predict(x_test)\n",
    "\n",
    "y_ne_pred = np.argmax(y_ne_pred, axis=2)\n",
    "y_pos_pred = np.argmax(y_pos_pred, axis=2)\n",
    "\n",
    "print(classification_report(y_ne_test.flatten(), y_ne_pred.flatten(), zero_division=0, target_names=unique_ne_tags))\n",
    "print(classification_report(y_pos_test.flatten(), y_pos_pred.flatten(), zero_division=0, target_names=unique_pos_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
