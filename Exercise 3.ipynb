{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 30 â€” Sep 3, 2021<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CSV file\n",
    "data = pd.read_csv(\"data/ner/gmb.csv\", encoding = 'latin1')\n",
    "\n",
    "# the first column of the file contains the sentence number\n",
    "# -- but only for the first token of each sentence.\n",
    "# The following line fills the rows downwards.\n",
    "data = data.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>POS_idx</th>\n",
       "      <th>NE_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>27700</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>20969</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>24218</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>26434</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>33389</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>9684</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>33464</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>29396</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>33246</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>34660</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>24853</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>8204</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "      <td>17364</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>20924</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>33246</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>34973</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>27700</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>4010</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>33786</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag  Word_idx  POS_idx  NE_idx\n",
       "1   Sentence: 1             of   IN      O     27700       10      16\n",
       "2   Sentence: 1  demonstrators  NNS      O     20969       19      16\n",
       "3   Sentence: 1           have  VBP      O     24218       35      16\n",
       "4   Sentence: 1        marched  VBN      O     26434       34      16\n",
       "5   Sentence: 1        through   IN      O     33389       10      16\n",
       "6   Sentence: 1         London  NNP  B-geo      9684       17       2\n",
       "7   Sentence: 1             to   TO      O     33464       29      16\n",
       "8   Sentence: 1        protest   VB      O     29396       31      16\n",
       "9   Sentence: 1            the   DT      O     33246        7      16\n",
       "10  Sentence: 1            war   NN      O     34660       16      16\n",
       "11  Sentence: 1             in   IN      O     24853       10      16\n",
       "12  Sentence: 1           Iraq  NNP  B-geo      8204       17       2\n",
       "13  Sentence: 1            and   CC      O     17364        5      16\n",
       "14  Sentence: 1         demand   VB      O     20924       31      16\n",
       "15  Sentence: 1            the   DT      O     33246        7      16\n",
       "16  Sentence: 1     withdrawal   NN      O     34973       16      16\n",
       "17  Sentence: 1             of   IN      O     27700       10      16\n",
       "18  Sentence: 1        British   JJ  B-gpe      4010       11       3\n",
       "19  Sentence: 1         troops  NNS      O     33786       19      16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of unique words and assign an integer number to it\n",
    "unique_words, coded_words = np.unique(data[\"Word\"], return_inverse=True)\n",
    "data[\"Word_idx\"] = coded_words\n",
    "EMPTY_WORD_IDX = len(unique_words)\n",
    "np.array(unique_words.tolist().append(\"_____\"))\n",
    "num_words = len(unique_words)+1\n",
    "\n",
    "unique_pos_tags, coded_pos_tags = np.unique(data[\"POS\"], return_inverse=True)\n",
    "data[\"POS_idx\"]  = coded_pos_tags\n",
    "NO_POS_TAG_IDX = len(unique_pos_tags)\n",
    "unique_pos_tags = unique_pos_tags.tolist()\n",
    "unique_pos_tags.append(\"_\")\n",
    "unique_pos_tags = np.array(unique_pos_tags)\n",
    "num_pos_tags = len(unique_pos_tags)\n",
    "\n",
    "\n",
    "# create a list of unique tags and assign an integer number to it\n",
    "unique_ne_tags, coded_ne_tags = np.unique(data[\"Tag\"], return_inverse=True)\n",
    "data[\"NE_idx\"]  = coded_ne_tags\n",
    "NO_NE_TAG_IDX = unique_ne_tags.tolist().index(\"O\")\n",
    "num_ne_tags = len(unique_ne_tags)\n",
    "\n",
    "# for verification and inspection, we can inspect the table so far\n",
    "data[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in sentence-wise processing.\n",
    "# Therefore, we use a function that gives us individual sentences.\n",
    "def get_sentences(data):\n",
    "  n_sent=1\n",
    "  agg_func = lambda s:[(w,p,t) \n",
    "    for w,p,t in zip(\n",
    "      s[\"Word_idx\"].values.tolist(),\n",
    "      s[\"POS_idx\"].values.tolist(),\n",
    "      s[\"NE_idx\"].values.tolist())]\n",
    "  grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "  return [s for s in grouped]\n",
    "\n",
    "sentences = get_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# find the maximum length for the sentences\n",
    "max_len = max([len(s) for s in sentences])\n",
    "\n",
    "# extract the word index\n",
    "x = [ [ w[0] for w in s ] for s in sentences ]\n",
    "\n",
    "# extract the tag index\n",
    "y_pos = [ [ w[1] for w in s ] for s in sentences ]\n",
    "y_ne = [ [ w[2] for w in s ] for s in sentences ]\n",
    "\n",
    "# shorter sentences are now padded to same length, using (index of) padding symbol\n",
    "x = pad_sequences(maxlen = max_len, sequences = x, \n",
    "  padding = 'post', value = EMPTY_WORD_IDX)\n",
    "\n",
    "# we do the same for the y data\n",
    "y_ne = pad_sequences(maxlen = max_len, sequences = y_ne, \n",
    "  padding = 'post', value = NO_NE_TAG_IDX)\n",
    "y_pos = pad_sequences(maxlen = max_len, sequences = y_pos, \n",
    "  padding = 'post', value = NO_POS_TAG_IDX)\n",
    "\n",
    "y_ne = np.array(y_ne)\n",
    "y_pos = np.array(y_pos)\n",
    "\n",
    "# but we also convert the indices to one-hot-encoding\n",
    "y_ne = to_categorical(y_ne, num_classes = num_ne_tags)\n",
    "y_pos = to_categorical(y_pos, num_classes = num_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_ne_train,y_ne_test,train_indices,test_indices = train_test_split(x, y_ne, range(len(x)), test_size = 0.1, random_state=1)\n",
    "\n",
    "y_pos_train = y_pos[train_indices]\n",
    "y_pos_test = y_pos[test_indices]\n",
    "\n",
    "y_train_weights = np.array([ [ 0.1 if w[len(w)-1] == 1 else 1 for w in s ]  for s in y_ne_train ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 104)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 104, 50)      1758950     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 104, 5)       1120        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ne (Dense)                      (None, 104, 17)      102         lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "pos (Dense)                     (None, 104, 43)      258         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 1,760,430\n",
      "Trainable params: 1,760,430\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "l_input = layers.Input(shape = (max_len,))\n",
    "l_embedding = layers.Embedding(input_dim = num_words, output_dim = 50, input_length = max_len)(l_input)\n",
    "l_lstm = layers.LSTM(units = 5, return_sequences = True)(l_embedding)\n",
    "l_output_ne = layers.Dense(num_ne_tags, name=\"ne\", activation = 'softmax')(l_lstm)\n",
    "l_output_pos = layers.Dense(num_pos_tags, name=\"pos\", activation = 'softmax')(l_lstm)\n",
    "\n",
    "model = models.Model(inputs = l_input, outputs=[l_output_ne, l_output_pos])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# We use a different optimizer this time\n",
    "model.compile(optimizer='Adam', \n",
    "  loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "675/675 [==============================] - 27s 38ms/step - loss: 2.3070 - ne_loss: 0.5412 - pos_loss: 1.7658 - ne_accuracy: 0.9670 - pos_accuracy: 0.7036\n",
      "Epoch 2/2\n",
      "675/675 [==============================] - 28s 42ms/step - loss: 0.8596 - ne_loss: 0.1538 - pos_loss: 0.7058 - ne_accuracy: 0.9678 - pos_accuracy: 0.8193\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, [np.array(y_ne_train), np.array(y_pos_train)],\n",
    "    batch_size = 64,\n",
    "    epochs = 2,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 2s 8ms/step - loss: 0.7328 - ne_loss: 0.1352 - pos_loss: 0.5976 - ne_accuracy: 0.9678 - pos_accuracy: 0.8493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7328184843063354,\n",
       " 0.13522876799106598,\n",
       " 0.5975896120071411,\n",
       " 0.9677796363830566,\n",
       " 0.8492814302444458]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, [y_ne_test, y_pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse one-hot-encoding for test data\n",
    "y_ne_test = np.argmax(y_ne_test, axis=2)\n",
    "y_pos_test = np.argmax(y_pos_test, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        48\n",
      "       B-eve       0.00      0.00      0.00        34\n",
      "       B-geo       0.00      0.00      0.00      3767\n",
      "       B-gpe       0.00      0.00      0.00      1607\n",
      "       B-nat       0.00      0.00      0.00        16\n",
      "       B-org       0.18      0.00      0.00      1948\n",
      "       B-per       0.00      0.00      0.00      1653\n",
      "       B-tim       0.00      0.00      0.00      2118\n",
      "       I-art       0.00      0.00      0.00        49\n",
      "       I-eve       0.00      0.00      0.00        30\n",
      "       I-geo       0.00      0.00      0.00       761\n",
      "       I-gpe       0.00      0.00      0.00        25\n",
      "       I-nat       0.00      0.00      0.00         6\n",
      "       I-org       0.00      0.00      0.00      1629\n",
      "       I-per       0.00      0.00      0.00      1695\n",
      "       I-tim       0.00      0.00      0.00       688\n",
      "           O       0.97      1.00      0.98    482710\n",
      "\n",
      "    accuracy                           0.97    498784\n",
      "   macro avg       0.07      0.06      0.06    498784\n",
      "weighted avg       0.94      0.97      0.95    498784\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           $       0.00      0.00      0.00       116\n",
      "           ,       0.00      0.00      0.00      3265\n",
      "           .       0.00      0.00      0.00      4795\n",
      "           :       0.00      0.00      0.00        90\n",
      "           ;       0.00      0.00      0.00        21\n",
      "          CC       0.00      0.00      0.00      2369\n",
      "          CD       0.00      0.00      0.00      2473\n",
      "          DT       0.00      0.00      0.00     10015\n",
      "          EX       0.00      0.00      0.00        69\n",
      "          FW       0.00      0.00      0.00         1\n",
      "          IN       0.00      0.00      0.00     12152\n",
      "          JJ       0.00      0.00      0.00      7873\n",
      "         JJR       0.00      0.00      0.00       295\n",
      "         JJS       0.00      0.00      0.00       291\n",
      "         LRB       0.00      0.00      0.00        83\n",
      "          MD       0.00      0.00      0.00       675\n",
      "          NN       0.31      0.90      0.47     14593\n",
      "         NNP       0.83      0.90      0.86     13069\n",
      "        NNPS       0.00      0.00      0.00       271\n",
      "         NNS       0.27      0.67      0.38      7589\n",
      "         PDT       0.00      0.00      0.00        21\n",
      "         POS       0.00      0.00      0.00      1100\n",
      "         PRP       0.00      0.00      0.00      1415\n",
      "        PRP$       0.00      0.00      0.00       913\n",
      "          RB       0.00      0.00      0.00      2023\n",
      "         RBR       0.00      0.00      0.00       111\n",
      "         RBS       0.00      0.00      0.00        26\n",
      "          RP       0.00      0.00      0.00       251\n",
      "         RRB       0.00      0.00      0.00        83\n",
      "          TO       0.00      0.00      0.00      2273\n",
      "          UH       0.00      0.00      0.00         5\n",
      "          VB       0.00      0.00      0.00      2341\n",
      "         VBD       0.00      0.00      0.00      4037\n",
      "         VBG       0.00      0.00      0.00      1937\n",
      "         VBN       0.00      0.00      0.00      3220\n",
      "         VBP       0.00      0.00      0.00      1624\n",
      "         VBZ       0.00      0.00      0.00      2449\n",
      "         WDT       0.00      0.00      0.00       355\n",
      "          WP       0.00      0.00      0.00       254\n",
      "         WP$       0.00      0.00      0.00         7\n",
      "         WRB       0.00      0.00      0.00       258\n",
      "          ``       0.00      0.00      0.00       399\n",
      "           _       0.93      1.00      0.96    393577\n",
      "\n",
      "    accuracy                           0.85    498784\n",
      "   macro avg       0.05      0.08      0.06    498784\n",
      "weighted avg       0.77      0.85      0.80    498784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_ne_pred, y_pos_pred = model.predict(x_test)\n",
    "\n",
    "y_ne_pred = np.argmax(y_ne_pred, axis=2)\n",
    "y_pos_pred = np.argmax(y_pos_pred, axis=2)\n",
    "\n",
    "print(classification_report(y_ne_test.flatten(), y_ne_pred.flatten(), zero_division=0, target_names=unique_ne_tags))\n",
    "print(classification_report(y_pos_test.flatten(), y_pos_pred.flatten(), zero_division=0, target_names=unique_pos_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
