{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 31 â€” Sep 4, 2020<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 3\n",
    "\n",
    "This exercise is about sequence labeling. A sequence of items (words, in this case) must the tagged with a sequence of labels. In this case the labels are named entity tags in the BIO scheme.\n",
    "\n",
    "The data we will be using comes from the Groningen Meaning Bank (GMB). Its annotation scheme can be found [here](http://www.let.rug.nl/bjerva/gmb/manual.php). As always, we will first preprocess the data, and then create and train the model.\n",
    "\n",
    "----\n",
    "\n",
    "Download the data, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/ner/gmb.csv ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/gmb.csv > data/ner/gmb.csv; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import into a pandas dataframe, and fill missing values. Also, let's look at the head of the table. We also directly encode the strings as integers, using the [numpy-function `np.unique(...)`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html). This will allow us to convert the index numbers back into readable tag strings later on.\n",
    "\n",
    "For padding (see below), we will be using `_____` as a \"word\", and `O` as a tag. `_____` needs to be added to the lists of unique words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in CSV file\n",
    "data = pd.read_csv(\"data/ner/gmb.csv\",encoding = 'latin1')\n",
    "\n",
    "# the first column of the file contains the sentence number\n",
    "# -- but only for the first token of each sentence.\n",
    "# The following line fills the rows downwards.\n",
    "data = data.fillna(method = 'ffill')\n",
    "\n",
    "# create a list of unique words and assign an integer number to it\n",
    "unique_words, coded_words = np.unique(data[\"Word\"], return_inverse=True)\n",
    "data[\"Word_idx\"] = coded_words\n",
    "EMPTY_WORD_IDX = len(unique_words)\n",
    "np.array(unique_words.tolist().append(\"_____\"))\n",
    "num_words = len(unique_words)+1\n",
    "\n",
    "# create a list of unique tags and assign an integer number to it\n",
    "unique_tags, coded_tags = np.unique(data[\"Tag\"], return_inverse=True)\n",
    "data[\"Tag_idx\"]  = coded_tags\n",
    "NO_TAG_IDX = unique_tags.tolist().index(\"O\")\n",
    "num_words_tag = len(unique_tags)\n",
    "\n",
    "# for verification and inspection, we print out the table so far\n",
    "data[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we convert the table in such a way that we can access individual sentences. The result of the function is a list of list of tuples, with the tuples containing the word, its part of speech tag and its named entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    n_sent=1\n",
    "    agg_func = lambda s:[(w,p,t) for w,p,t in zip(s[\"Word_idx\"].values.tolist(),\n",
    "                                                     s[\"POS\"].values.tolist(),\n",
    "                                                     s[\"Tag_idx\"].values.tolist())]\n",
    "    grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "    return [s for s in grouped]\n",
    "\n",
    "\n",
    "sentences = get_sentences(data)\n",
    "\n",
    "# print out the first sentence for verification\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, we pad the sequences to the length of the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# find the maximum length for the sentences\n",
    "max_len = max([len(s) for s in sentences])\n",
    "\n",
    "x = np.array([ np.array([ w[0] for w in s ]) for s in sentences ])\n",
    "y = np.array([ np.array([ w[2] for w in s ]) for s in sentences ])\n",
    "\n",
    "# shorter sentences are now padded to same length, using (index of) padding symbol\n",
    "x = pad_sequences(maxlen = max_len, sequences = x, padding = 'post', value = EMPTY_WORD_IDX)\n",
    "\n",
    "# we do the same for the y data\n",
    "y = pad_sequences(maxlen = max_len, sequences = y, padding = 'post', value = NO_TAG_IDX)\n",
    "\n",
    "# but we also convert the indices to keras categories\n",
    "y = np.array([to_categorical(i, num_classes = num_words_tag) for i in  y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into trainig and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the model architecture.\n",
    "\n",
    "Things to try:\n",
    "- Pretrained embeddings\n",
    "- Dropout/Regularization\n",
    "- Bidirectionality\n",
    "- More dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape = (max_len,)))\n",
    "model.add(layers.Embedding(input_dim = num_words, output_dim = 1, input_length = max_len))\n",
    "model.add(layers.SimpleRNN(units = 5, return_sequences = True))\n",
    "model.add(layers.Dense(num_words_tag, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, np.array(y_train),\n",
    "    batch_size = 128,\n",
    "    epochs = 1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation by class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have mostly looked at accuracy scores. For this task, however, this may not giving us the entire picture, because there are many different target classes, and the model might perform differently for them. So look at an evaluation by class. For this, the [function `classification_report(...)` from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_test = np.argmax(y_test, axis=2)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=2)\n",
    "\n",
    "\n",
    "print(classification_report(Y_test.flatten(), y_pred.flatten(), zero_division=0, target_names=unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "! We will do this on Thursday, 9:30 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/ner/challenge.wb.csv ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.wb.csv > data/ner/challenge.wb.csv; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/ner/challenge.bc.csv ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.bc.csv > data/ner/challenge.bc.csv; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/ner/challenge.nw.csv ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/ner/challenge.nw.csv > data/ner/challenge.nw.csv; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = { w : i for i, w in enumerate( unique_words ) }\n",
    "tag_index = { t : i for i, t in enumerate( unique_tags ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = pd.read_csv(\"data/ner/challenge.wb.csv\", header = 0, names=[\"Sentence #\",\"Word\",\"POS\",\"Tag\"])\n",
    "\n",
    "challenge[\"Word_idx\"] = [ word_index.get(w, EMPTY_WORD_IDX) for w in challenge[\"Word\"]]\n",
    "challenge[\"Tag_idx\"] = [ tag_index[w] for w in challenge[\"Tag\"]]\n",
    "\n",
    "sentences = get_sentences(challenge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_challenge = [[w[0] for w in s] for s in sentences]\n",
    "x_challenge = pad_sequences(maxlen = max_len,sequences = x_challenge,padding = 'post',value = EMPTY_WORD_IDX)\n",
    "y_challenge = [[w[2] for w in s] for s in sentences]\n",
    "y_challenge = pad_sequences(maxlen = max_len,sequences = y_challenge,padding =\n",
    "                        'post',value = tag_index['O'])\n",
    "y_challenge = [to_categorical(i, num_classes = num_words_tag) for i in  y_challenge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_challenge, np.array(y_challenge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_test = np.argmax(y_challenge, axis=2)\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_challenge), axis=2)\n",
    "\n",
    "print(classification_report(Y_test.flatten(), y_pred.flatten(), zero_division=0, labels=range(len(unique_tags)), target_names=unique_tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
