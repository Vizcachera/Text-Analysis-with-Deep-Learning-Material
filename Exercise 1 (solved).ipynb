{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"http://ml-school.uni-koeln.de\">Virtual Summer School \"Deep Learning for\n",
    "    Language Analysis\"</a> <br/><strong>Text Analysis with Deep Learning</strong><br/>Aug 31 â€” Sep 4, 2020<br/>Nils Reiter<br/><a href=\"mailto:nils.reiter@uni-koeln.de\">nils.reiter@uni-koeln.de</a></div>\n",
    "\n",
    "# Exercise 1: Sentiment analysis as bag of words\n",
    "\n",
    "This is the first exercise for you to solve independently, but as a group of approximately three students. Feel free to contact us via [Teams](https://teams.microsoft.com/l/team/19%3aeefdaf656d5d48d3868e04682d159f57%40thread.tacv2/conversations?groupId=de73deca-e22a-46d1-81da-42a4e12897f9&tenantId=4982814a-1107-493e-ac2f-3356509a8687) and call us into your room if you need support. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import numpy as np\n",
    "\n",
    "def get_labels_and_texts(file, limit=100000):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    lineNumber = 0\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "        lineNumber = lineNumber + 1\n",
    "        if lineNumber >= limit and limit > 0:\n",
    "          break\n",
    "    return np.array(labels), texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `data/amazon/train.ft.txt.bz2` does not exist, we download it. (The exclamation mark `!` indicates that the command is executed not by Python, but by the underlying shell. That's why the syntax does not look like python at all.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/amazon/train.ft.txt.bz2 ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/amazon/train.ft.txt.bz2 > data/amazon/train.ft.txt.bz2; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `data/amazon/test.ft.txt.bz2` does not exist, we download it. (The exclamation mark `!` indicates that the command is executed not by Python, but by the underlying shell. That's why the syntax does not look like python at all.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line opens and parses the file we have downloaded before. The function `get_labels_and_texts(...)` is defined above. Because we are not overwriting the argument `limit`, the function only loads the first 100000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/amazon/train.ft.txt.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-109a65d55947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels_and_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/amazon/train.ft.txt.bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-e5206ab2485e>\u001b[0m in \u001b[0;36mget_labels_and_texts\u001b[0;34m(file, limit)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlineNumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/amazon/train.ft.txt.bz2'"
     ]
    }
   ],
   "source": [
    "train_labels, train_texts = get_labels_and_texts('data/amazon/train.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have imported the train and test data into variables. `train_labels` contains the classes, `train_texts` the corresponding reviews. Feel free to inspect those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# shows the seventh text (note the typo!)\n",
    "train_texts[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "In this exercise, we want the input to be a document-term-matrix. I.e., each document is represented by a numeric vector. The vector contains one dimension for each token in the vocabulary, i.e., unique token in the entire (training) corpus (we will talk about more in-depth this on Tuesday).\n",
    "\n",
    "As an example, consider the following matrix:\n",
    "\n",
    "| document | dog | cat | mouse | the | a | an |\n",
    "| --- | --- | --- | --- | --- | --- | --- | \n",
    "| d1 | 5 | 6 | 0 | 10 | 5 | 6 |\n",
    "| d2 | 0 | 1 | 10 | 3 | 1 | 0 |\n",
    "| ... | ... | ... | ... | ... | ... | ... | \n",
    "\n",
    "Document `d1` contains five occurrences of the word \"dog\", six of the word \"cat\" etc.\n",
    "\n",
    "Creating such a matrix requires creating a vocabulary and then counting all these words. The class `CountVectorizer()` from `scikit-learn` is exactly what we need for this. You'll find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Please use it to create a document-term matrix for the `train_texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "train_texts_vec = vectorizer.transform(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "The vectors for each document are now represented as sparse arrays, i.e., they are not fully realized (zeros are not stored, for instance). To make them dense, we can use the `numpy`-function `todense()`. This is also a good opportunity to limit the number of training instances for development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numInstances = 10000\n",
    "\n",
    "x_train = train_texts_vec[:numInstances].todense()\n",
    "y_train = train_labels[:numInstances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "We are now ready to define the neural network. Please define a neural network with \n",
    "1. an input layer with an appropriate `shape` argument\n",
    "2. an hidden layer with size 5 and activation function `sigmoid`\n",
    "3. an output layer with activation function `sigmoid`\n",
    "\n",
    "One you have successfully fitted a first model, try to change the architecture to improve its accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "ffnn = models.Sequential()\n",
    "ffnn.add(layers.Input(shape=(5000,)))\n",
    "ffnn.add(layers.Dense(5, activation=\"sigmoid\"))\n",
    "ffnn.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "ffnn.compile(loss=\"mean_squared_error\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model `ffnn` can now be trained on the input data, using the function `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ffnn.fit(x_train, y_train, epochs=10, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation\n",
    "\n",
    "Now that the model has been trained, we can test it on held-out data.\n",
    "To this end, you can download a data set with the shell command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if ! [[ -f data/amazon/test.ft.txt.bz2 ]]; then curl https://nilsreiter.de/assets/2020-08-31-deep-learning/amazon/test.ft.txt.bz2 > data/amazon/test.ft.txt.bz2; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual evaluation, the test dataset needs to undergo the same preprocessing steps as the training data.\n",
    "1. Read in the data from a file\n",
    "2. Vectorize each document, using the same vectorizer (from above)\n",
    "3. Create one matrix `x_test` that contains the input data and one array `y_test` that contains the labels.\n",
    "\n",
    "Use the Keras-function `evaluate()` on the model ([documentation](https://keras.io/api/models/model_training_apis/#evaluate-method))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_texts = get_labels_and_texts('data/amazon/test.ft.txt.bz2')\n",
    "test_texts_vec = vectorizer.transform(test_texts)\n",
    "\n",
    "x_test = test_texts_vec[:numInstances].todense()\n",
    "y_test = test_labels[:numInstances]\n",
    "\n",
    "ffnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This notebook is based on [this one](https://www.kaggle.com/muonneutrino/sentiment-analysis-with-amazon-reviews) by MuonNeutrino on kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
